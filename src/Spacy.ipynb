{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "sents = nlp(u'A woman is walking through the door.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "import time\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0016591548919677734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "I"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "begin=time.time()\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "begin=time.time()\n",
    "doc = nlp('I like green eggs and ham.A woman is walking through the door.')\n",
    "\n",
    "#for np in doc.noun_chunks:\n",
    "#    print(np.text, np.root.text, np.root.dep_, np.root.head.text)\n",
    "    # I I nsubj like\n",
    "    # green eggs eggs dobj like\n",
    "    # ham ham conj eggs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0013890266418457031\n"
     ]
    }
   ],
   "source": [
    "begin=time.time()\n",
    "doc = nlp('I like green eggs and ham.A woman is walking through the door.')\n",
    "tokenlist=[]\n",
    "for token in doc:\n",
    "    tokenlist.append(token)\n",
    "print(time.time()-begin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0003941059112548828\n"
     ]
    }
   ],
   "source": [
    "begin=time.time()\n",
    "token=nltk.word_tokenize('I like green eggs and ham.A woman is walking through the door.')\n",
    "print(time.time()-begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lcs(xstr, ystr):\n",
    "    if not xstr or not ystr:\n",
    "        return 0\n",
    "    x, xs, y, ys = xstr[0], xstr[1:], ystr[0], ystr[1:]\n",
    "    if x == y:\n",
    "        return 1 + lcs(xs, ys)\n",
    "    else:\n",
    "        return max(lcs(xstr, ys), lcs(xs, ystr))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001499176025390625\n"
     ]
    }
   ],
   "source": [
    "begin=time.time()\n",
    "doc = nlp('I like green eggs and ham.A woman is walking through the door.')\n",
    "lcs(doc,doc)\n",
    "print(time.time()-begin)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0006279945373535156\n"
     ]
    }
   ],
   "source": [
    "begin=time.time()\n",
    "token=nltk.word_tokenize('I like green eggs and ham. A woman is walking through the door.')\n",
    "lcs(token,token)\n",
    "print(time.time()-begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2=nltk.word_tokenize('I dont like red eggs and ham. A man is walking through the door.')\n",
    "nltk.edit_distance(token,token2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "I like green eggs and ham.\n",
      "A woman is walking through the door.\n"
     ]
    }
   ],
   "source": [
    "# Get first token of the processed document\n",
    "token = doc[0]\n",
    "print(token)\n",
    "\n",
    "# Print sentences (one sentence per line)\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I - PRON\n",
      "like - VERB\n",
      "green - ADJ\n",
      "eggs - NOUN\n",
      "and - CCONJ\n",
      "ham - NOUN\n",
      ". - PUNCT\n",
      "A - DET\n",
      "woman - NOUN\n",
      "is - VERB\n",
      "walking - VERB\n",
      "through - ADP\n",
      "the - DET\n",
      "door - NOUN\n",
      ". - PUNCT\n"
     ]
    }
   ],
   "source": [
    "# For each token, print corresponding part of speech tag\n",
    "for token in doc:\n",
    "    print('{} - {}'.format(token, token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I --> [I, like]\n",
      "like --> []\n",
      "green --> [green, eggs, eggs, like]\n",
      "eggs --> [eggs, like]\n",
      "and --> [and, eggs, eggs, like]\n",
      "ham --> [ham, eggs, eggs, like]\n",
      ". --> [., like]\n",
      "A --> [A, woman, woman, walking]\n",
      "woman --> [woman, walking]\n",
      "is --> [is, walking]\n",
      "walking --> []\n",
      "through --> [through, walking]\n",
      "the --> [the, door, door, through, through, walking]\n",
      "door --> [door, through, through, walking]\n",
      ". --> [., walking]\n",
      "I-nsubj-> like-ROOT\n",
      "\n",
      "green-amod-> eggs-dobj-> eggs-dobj-> like-ROOT\n",
      "eggs-dobj-> like-ROOT\n",
      "and-cc-> eggs-dobj-> eggs-dobj-> like-ROOT\n",
      "ham-conj-> eggs-dobj-> eggs-dobj-> like-ROOT\n",
      ".-punct-> like-ROOT\n",
      "A-det-> woman-nsubj-> woman-nsubj-> walking-ROOT\n",
      "woman-nsubj-> walking-ROOT\n",
      "is-aux-> walking-ROOT\n",
      "\n",
      "through-prep-> walking-ROOT\n",
      "the-det-> door-pobj-> door-pobj-> through-prep-> through-prep-> walking-ROOT\n",
      "door-pobj-> through-prep-> through-prep-> walking-ROOT\n",
      ".-punct-> walking-ROOT\n"
     ]
    }
   ],
   "source": [
    "#Write a function that walks up the syntactic tree of the given token and collects all tokens to the root token (including root token).\n",
    "\n",
    "def tokens_to_root(token):\n",
    "    \"\"\"\n",
    "    Walk up the syntactic tree, collecting tokens to the root of the given `token`.\n",
    "    :param token: Spacy token\n",
    "    :return: list of Spacy tokens\n",
    "    \"\"\"\n",
    "    tokens_to_r = []\n",
    "    while token.head is not token:\n",
    "        tokens_to_r.append(token)\n",
    "        token = token.head\n",
    "        tokens_to_r.append(token)\n",
    "\n",
    "    return tokens_to_r\n",
    "\n",
    "# For every token in document, print it's tokens to the root\n",
    "for token in doc:\n",
    "    print('{} --> {}'.format(token, tokens_to_root(token)))\n",
    "\n",
    "# Print dependency labels of the tokens\n",
    "for token in doc:\n",
    "    print('-> '.join(['{}-{}'.format(dependent_token, dependent_token.dep_) for dependent_token in tokens_to_root(token)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOC 1\n",
      "JAY - PERSON\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print all named entities with named entity types\n",
    "doc_1 = nlp('WHO IS JAY?')\n",
    "doc_2 = nlp('who is jay?')\n",
    "for ent in doc_2.ents:\n",
    "    print('DOC 2')\n",
    "    print('{} - {}'.format(ent, ent.label_))\n",
    "for ent in doc_1.ents:\n",
    "    print('DOC 1')\n",
    "    print('{} - {}'.format(ent, ent.label_))\n",
    "    \n",
    "len(doc_1.ents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I, Paris, I, my old friend, uni]\n"
     ]
    }
   ],
   "source": [
    "# Print noun chunks for doc_2\n",
    "print([chunk for chunk in doc_2.noun_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I , -4.064180850982666\n",
      "went , -8.474893569946289\n",
      "to , -3.83851957321167\n",
      "Paris , -11.6917724609375\n",
      "where , -7.183883190155029\n",
      "I , -4.064180850982666\n",
      "met , -9.784490585327148\n",
      "my , -5.918124675750732\n",
      "old , -7.7954816818237305\n",
      "friend , -8.825821876525879\n",
      "Jack , -11.20296573638916\n",
      "from , -6.028810501098633\n",
      "uni , -19.579313278198242\n",
      ". , -3.0729479789733887\n"
     ]
    }
   ],
   "source": [
    "# For every token in doc_2, print log-probability of the word, estimated from counts from a large corpus \n",
    "for token in doc_2:\n",
    "    print(token, ',', token.prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "\n",
      "0.569403101179\n",
      "0.323890751106\n",
      "0.832116326852\n"
     ]
    }
   ],
   "source": [
    "# For a given document, calculate similarity between 'apples' and 'oranges' and 'boots' and 'hippos'\n",
    "doc = nlp(u\"Apples and oranges are similar. Boots and hippos aren't.\")\n",
    "apples = doc[0]\n",
    "oranges = doc[2]\n",
    "boots = doc[6]\n",
    "hippos = doc[8]\n",
    "print(apples.similarity(oranges))\n",
    "print(boots.similarity(hippos))\n",
    "\n",
    "print()\n",
    "# Print similarity between sentence and word 'fruit'\n",
    "apples_sent, boots_sent = doc.sents\n",
    "fruit = doc.vocab[u'fruit']\n",
    "print(apples_sent.similarity(fruit))\n",
    "print(boots_sent.similarity(fruit))\n",
    "print(boots_sent.similarity(apples_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of nltk dependency grammar \n",
    "#### not useful for dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency grammar with 7 productions\n",
      "  'shot' -> 'I'\n",
      "  'shot' -> 'elephant'\n",
      "  'shot' -> 'in'\n",
      "  'elephant' -> 'an'\n",
      "  'elephant' -> 'in'\n",
      "  'in' -> 'pajamas'\n",
      "  'pajamas' -> 'my'\n"
     ]
    }
   ],
   "source": [
    "groucho_dep_grammar = nltk.DependencyGrammar.fromstring(\"\"\"\n",
    "'shot' -> 'I' | 'elephant' | 'in'\n",
    "'elephant' -> 'an' | 'in'\n",
    "'in' -> 'pajamas'\n",
    "'pajamas' -> 'my'\n",
    "\"\"\")\n",
    "print(groucho_dep_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(shot I (elephant an (in (pajamas my))))\n",
      "(shot I (elephant an) (in (pajamas my)))\n"
     ]
    }
   ],
   "source": [
    "pdp = nltk.ProjectiveDependencyParser(groucho_dep_grammar)\n",
    "sent = 'I shot an elephant in my pajamas'.split()\n",
    "trees = pdp.parse(sent)\n",
    "for tree in trees:\n",
    "     print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
